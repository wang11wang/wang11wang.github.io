---
title: InternVLç³»åˆ—è§£è¯»
date: 2025-02-03
tags:
  - InternVL
  - MLLM
---
InternVLç³»åˆ—æ•™ç¨‹ï¼š<https://internvl.readthedocs.io/en/latest/index.html>
# InternVL
å‘è¡¨äº2023.12

Paper: [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/pdf/2312.14238)

Tutorial: <https://internvl.readthedocs.io/en/latest/internvl1.0/classification.html>
![internvl-arch](internvl-arch.png)



## è®­ç»ƒç­–ç•¥ï¼š
### Stage-1: Vision-Language Contrastive Training
Visual Encoder: InterViT-6Bï¼Œéšæœºåˆå§‹åŒ–æƒé‡

LLM: å¤šè¯­è¨€çš„LLaMA-7Bï¼ˆç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„æƒé‡ï¼‰

æŸå¤±ï¼šå¯¹ç§°çš„å›¾æ–‡å¯¹æ¯”æŸå¤±

æ•°æ®ï¼šweb scale, noisy image-text pairsï¼Œå°†6.03Bå›¾æ–‡å¯¹æ¸…æ´—ä¹‹åè¿˜å‰©ä¸‹4.98B

Visual Encoderå’ŒLLMçš„æƒé‡éƒ½æ›´æ–°
### Stage-2: Vision-Language Generative Training
QLLaMAï¼šä½¿ç”¨Stage-1ä¸­çš„LLaMA-7Bçš„æƒé‡

å†»ç»“Visual Encoderå’ŒLLMçš„æƒé‡ï¼Œåªæ›´æ–°**æ–°å¢åŠ çš„æƒé‡ï¼šlearnable queries and cross attention layers**

æ•°æ®ï¼šå°†ä¸Šè¿°4.98Bæ•°æ®è¿›ä¸€æ­¥æ¸…æ´—ï¼Œå¾—åˆ°1.03Bæ•°æ®

æŸå¤±ï¼šç±»ä¼¼BLIP2çš„image-text contrastive loss(ITC)ï¼Œimage-text match loss(ITM)å’Œimage-grounded text generation loss(ITG)
### Stage-3: SFT
æ•°æ®ï¼šå¤§çº¦4Mçš„é«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®

è¿æ¥LLMï¼š 2ç§è¿æ¥æ–¹å¼ï¼š

    1. å¯ä»¥å°†å¦å¤–ä¸€ä¸ªLLMï¼ˆå¦‚Vicuna-13Bæˆ–è€…InternLLMï¼‰é€šè¿‡ä¸€ä¸ªMLPè¿æ¥åˆ°Visual Encoderä¸Š
    2. å°†LLMé€šè¿‡MLPè¿æ¥åˆ°QLLaMAä¸Šï¼ŒQLLaMAçš„æƒé‡ä¸éœ€è¦æ›´æ–°
è®­ç»ƒï¼šå¯ä»¥åªæ›´æ–°MLPçš„æƒé‡ï¼Œä¹Ÿå¯ä»¥æ›´æ–°MLP + LLMçš„æƒé‡

# InternVL-1.1
å‘è¡¨äº2024.01.24 Blog: [InternVL 1.1: Enhance Chinese and OCR Capabilities](https://internvl.github.io/blog/2024-01-24-InternVL-1.1)

Tutorial: <https://internvl.readthedocs.io/en/latest/internvl1.1/introduction.html>
![internvl-1-1](internvl-1-1-arch.png)

å‘å¸ƒäº†InternVL-Chat-V1-1 å’Œ InternViT-6B-448px-V1-0 2ä¸ªæ¨¡å‹

æ¨¡å‹æ¶æ„ç±»ä¼¼LLaVAï¼Œä½¿ç”¨MLPè¿æ¥InternViT-6Bå’ŒLLaMA2-13Bï¼Œå½¢æˆ1ä¸ª19Bçš„æ¨¡å‹

åˆ†è¾¨ç‡å˜åŒ–ï¼šå›¾åƒè¾“å…¥å˜æˆ448 x 448ï¼Œä¼šç”Ÿæˆ1024ä¸ªtokenï¼Œä½¿ç”¨Pixel Shuffleé™ä½åˆ°256ä¸ªtokenï¼›

å¢å¼ºOCRèƒ½åŠ›å’Œä¸­æ–‡èƒ½åŠ›

## è®­ç»ƒ
### Stage-1: Pretraining 
åªæ›´æ–°ViTå’ŒMLPçš„æƒé‡ï¼ŒViTä½¿ç”¨InternVL-1.0çš„ä¸­çš„InternViT-6Bï¼ˆ224 x 224)çš„æƒé‡åˆå§‹åŒ–
### Stage-2: SFT
åªæ›´æ–°MLPå’ŒLLMçš„æƒé‡
# InternVL-1.2
å‘è¡¨äº2024.02.12ï¼Œæ¯”LLaVA-NeXTç¨æ™šä¸€ç‚¹ï¼Œå—åˆ°LLaVA-NeXT-34Bçš„å¯å‘ï¼Œä½¿ç”¨äº†æ›´å¤§çš„LLM: [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B)

Blog: [InternVL 1.2: Scaling up LLM to 34B](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/)

Tutorial: <https://internvl.readthedocs.io/en/latest/internvl1.2/introduction.html>

![internVL-1-2-arch](internVL-1-2-arch.png)

## è®­ç»ƒ
### Stage-1: Pretraining
å’ŒInternVL-1.1ç›¸åŒï¼Œåªæ›´æ–°ViTå’ŒMLPçš„æƒé‡
### Stage-2: SFT
æ­¤å¤„å’ŒInterVL-1.1ä¸åŒï¼Œå…¨æ¨¡å‹è®­ç»ƒï¼ˆ40Bï¼‰

Model Card
InternVL-Chat-V1-2ä½¿ç”¨çš„æ˜¯1.2Mçš„SFTæ•°æ®ï¼ŒInternVL-Chat-V1-2-Plusä½¿ç”¨çš„æ˜¯12Mçš„SFTæ•°æ®
<table>
    <thead>
        <tr>
            <th>Type</th>
            <th>Model</th>
            <th>Date</th>
            <th>Download</th>
            <th>Note</th>
        </tr>
    </thead>
<tbody>                                         
<tr>
    <td rowspan="2">Vision Large Language Model</td>
    <td>InternVL-Chat-V1-2-Plus</td>
    <td>2024.02.21</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus" rel="nofollow">HF link</a></td>
    <td>more SFT data and stronger</td>
</tr>
<tr>
    <td>InternVL-Chat-V1-2</td>
    <td>2024.02.11</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2" rel="nofollow">HF link</a></td>
    <td>scaling up LLM to 34B</td>
</tr>
<tr>
    <td>Vision Foundation Model</td>
    <td>InternViT-6B-448px-V1-2</td>
    <td>2024.01.30</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2" rel="nofollow">HF link</a></td>
    <td>vision foundation model, 448 resolution</td>
</tr>
</tbody>
</table>

# InternVL-1.5
å‘è¡¨äº 2024.04.25

Paper: [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821)

Blog: [InternVL 1.5: How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://internvl.github.io/blog/2024-04-30-InternVL-1.5/)
## Model Card
<table>
    <thead>
    <tr>
    <th>Type</th>
    <th>Model</th>
    <th>Date</th>
    <th>Download</th>
    <th>Note</th>
    </tr>
    </thead>
    <tbody>
<tr>
    <td rowspan="2">Vision Large Language Model</td>
    <td>InternVL-Chat-V1-5-Int8</td>
    <td>2024.04.28</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8" rel="nofollow">HF link</a></td>
    <td>The INT8 version of InternVL-Chat-V1-5</td>
</tr>
<tr>
    <td><nobr>InternVL-Chat-V1-5</nobr></td>
    <td>2024.04.18</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5" rel="nofollow">HF link</a></td>
    <td>support 4K image; super strong OCR; Approaching the performance of GPT-4V and Gemini Pro on various benchmarks like MMMU, DocVQA, ChartQA, MathVista, etc. (ğŸ”¥new)</td>
</tr>
<tr>
        <td>Vision Foundation Model</td>
        <td><nobr>InternViT-6B-448px-V1-5</nobr></td>
    <td>2024.04.20</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5" rel="nofollow">HF link</a></td>
    <td>support dynamic resolution, super strong OCR (ğŸ”¥new)</td>
</tr>
</tbody>
</table>

![internVL-1.5-arch](internVL-1.5-arch.png)

## åŠ¨æ€é«˜åˆ†è¾¨ç‡
æŒ‰ç…§aspect ratioå°†åŸå§‹å›¾åˆ†æˆå¤šä¸ª448 x 448çš„patches + åŸå§‹å›¾çš„ç¼©æ”¾åˆ°448 x 448

è®­ç»ƒçš„æ—¶å€™ï¼šä½¿ç”¨æœ€å¤š12ä¸ªï¼›
æ¨ç†çš„æ—¶å€™ï¼šæœ€å¤šå¯ä»¥ä½¿ç”¨40ä¸ªï¼ˆæ”¯æŒ4Kåˆ†è¾¨ç‡ï¼‰

ä½¿ç”¨Pixel Shuffleé™ä½å°†å•ä¸ªpatchäº§ç”Ÿçš„tokenæ•°ä»1024é™ä½åˆ°256
ï¼Œå³448 x 448çš„patch/imageä½¿ç”¨256ä¸ªtokenè¡¨ç¤º

## è®­ç»ƒ
### InternVL-Chat-V1-5 (26B)
LLM: InternLM2-20B

Stage-1: é¢„è®­ç»ƒ ViT + MLP

Stage-2: 5Mé«˜è´¨é‡çš„åŒè¯­æ•°æ®ï¼Œå’ŒInterVL-1.2ç›¸åŒï¼ŒViT + MLP + LLM éƒ½è¿›è¡Œå¾®è°ƒ

### InternVL-Chat-V1-5-Plusï¼ˆ40Bï¼‰
LLM: [Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B)

Stage-1: é¢„è®­ç»ƒé˜¶æ®µåªå¾®è°ƒ MLP

Stage-2: åŒInternVL-Chat-V1-5ï¼Œä½¿ç”¨5Mé«˜è´¨é‡çš„åŒè¯­æ•°æ®ï¼Œå¯¹ViT + MLP + LLMéƒ½å¾®è°ƒ

# Mini-InterVL 1.5
å‘è¡¨äº2024.05.25çš„Blog: [Mini-InternVL 1.5: A Powerful Pocket Multimodal Model with 8% Parameters for 80% Performance](https://internvl.github.io/blog/2024-05-25-Mini-InternVL-1.5)

## Model Card
<table>
    <thead>
    <tr>
    <th>Type</th>
    <th>Model</th>
    <th>Date</th>
    <th>Download</th>
    <th>Note</th>
    </tr>
    </thead>
<tbody>
<tr>
    <td rowspan="2">Vision Large Language Model</td>
    <td>Mini-InternVL-Chat-2B-V1-5</td>
    <td>2024.05.19</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5" rel="nofollow">HF link</a></td>
    <td>ğŸš€ğŸš€ Only 2B parameters, anyone can deploy it locally.</td>
</tr>
<tr>
    <td>Mini-InternVL-Chat-4B-V1-5</td>
    <td>2024.05.28</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5" rel="nofollow">HF link</a></td>
    <td>ğŸš€ Only 4B parameters, anyone can deploy it locally.</td>
</tr>
<tr>
    <td>Vision Foundation Model</td>
    <td>InternViT-300M-448px</td>
    <td>2024.05.25</td>
    <td>ğŸ¤— <a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px" rel="nofollow">HF link</a></td>
    <td>Distilled small vision foundation model with 300M parameters.</td>
</tr>
</tbody>
</table>

## æ¨¡å‹ç»“æ„
ç»“æ„å’ŒInternVL-1.5ç›¸åŒ
### Visual Encoder
å°†InternViT-6B-448px-V1-5è’¸é¦åˆ°300Mï¼Œå³InternViT-300M-448px
### LLM
InternLM2-Chat-1.8B æˆ–è€… Phi-3-mini-128k-instruct (3.8B)
### è®­ç»ƒæ–¹æ³•
å’ŒInternVL-1.5ç±»ä¼¼ï¼Œä½¿ç”¨åŒæ ·çš„æ•°æ®ï¼Œ2Bçš„æ¨¡å‹å’Œ26Bçš„æ¨¡å‹è®­ç»ƒæ–¹å¼ä¸€æ ·ï¼Œ4Bçš„æ¨¡å‹å’Œ40Bçš„è®­ç»ƒæ–¹æ³•ä¸€æ ·

# InternVL-2.0
å‘è¡¨äº2024.07.02çš„Blogï¼š[InternVL2: Better than the Bestâ€”Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)

æ¨¡å‹å¤§å°æœ‰ï¼š1B, 2B, 4B, 8B, 26B, 40B, 76B, 108Bï¼Œå…¶ä¸­8BåŠä»¥ä¸‹çš„æ¨¡å‹ä½¿ç”¨InternViT-300M-448pxï¼Œ26BåŠä»¥ä¸Šçš„æ¨¡å‹ä½¿ç”¨InternViT-6B-448px-V1-5

æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼Œè§†é¢‘ï¼ŒåŒ»ç–—æ•°æ®ï¼‰ï¼Œå¤šä»»åŠ¡è¾“å‡ºï¼ˆå›¾ï¼Œbboxï¼Œmaskï¼‰

## è®­ç»ƒæ–¹æ³•
### Stage-1
åœ¨InternVL-1.5çš„æ•°æ®ä¸Šåšäº†æ‰©å……ï¼Œåªå¾®è°ƒMLP
### Stage-2
InternVL-1.5çš„5Mé«˜è´¨é‡çš„åŒè¯­æ•°æ®
 ViT + MLP + LLM

Progressive with larger language modelsï¼Ÿ

# InternOmini
å‘è¡¨äº2024.07.27çš„Blog: [InternOmni: Extending InternVL with Audio Modality](https://internvl.github.io/blog/2024-07-27-InternOmni/)ï¼Œå¢åŠ äº†å¯¹Audioçš„å¤„ç†

# Mono-InternVL
å‘è¡¨äº2024.10.10 

Paper: [Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training](https://arxiv.org/abs/2410.08202)

Blog: [Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training](https://internvl.github.io/blog/2024-10-10-Mono-InternVL/)

TODOï¼Œè¯¦ç»†å†…å®¹ä»¥åå†è¯´

# Mini-InternVL-2.0
å‘è¡¨äº2024.10.21

Paper: [Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance](https://arxiv.org/abs/2410.16261)

Blog: [Mini-InternVL 2.0: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance
](https://internvl.github.io/blog/2024-10-21-Mini-InternVL-2.0)

å‘å¸ƒäº†1B, 2Bå’Œ4Bçš„æ¨¡å‹ï¼Œå…¶ä¸­<font color=red>4Bçš„æ¨¡å‹ä½¿ç”¨5%çš„å‚æ•°å®ç°äº†InternVL2-Llama3-76B 90%çš„æ€§èƒ½</font>

![mini-internvl-2.0-arch](mini-internvl-2.0-arch.png)
ä½¿ç”¨CLIP-ViT-L/336px(300M)åˆå§‹åŒ–InternViT-300Mï¼Œç„¶åä½¿ç”¨InternViT-6Bå°†çŸ¥è¯†è’¸é¦åˆ°InternViT-300M

InternViT-300Mè¾“å…¥æ˜¯448pxï¼Œé‡‡ç”¨åŠ¨æ€é«˜åˆ†è¾¨ç‡ï¼Œæ¯ä¸ª448 x 448çš„patchäº§ç”Ÿ1024ä¸ªtokenï¼Œç»è¿‡Pixel Unshuffleé™ä½åˆ°256ä¸ªToken

## è®­ç»ƒ
### Stage-1: 
åœ¨InternVL-1.5çš„æ‰©å±•æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¹äº1Bå’Œ2Bæ¨¡å‹è®­ç»ƒViT + MLPï¼Œå¯¹äº4Bçš„æ¨¡å‹ï¼Œåªè®­ç»ƒMLP
### Stage-2:
ä½¿ç”¨InterVL-1.5çš„5Mé«˜è´¨é‡çš„åŒè¯­æ•°æ®ï¼Œå¯¹æ•´ä¸ªæ¨¡å‹çš„å‚æ•°éƒ½åšæ›´æ–°ï¼ˆå³ViT + MLP + LLMï¼‰

# InternVL-2.5
å‘è¡¨äº2024.12.6

Paper: [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271)

Blog: [InternVL2.5: Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://internvl.github.io/blog/2024-12-05-InternVL-2.5)

æ¨¡å‹ç»“æ„å’ŒInternVL-1.5ï¼ŒInternVL-2.0éƒ½ä¸€æ ·

ä¸åŒç±»å‹çš„æ•°æ®ï¼Œä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ•°æ®æ ¼å¼
![internvl-2.5-data-format](internvl-2.5-data-format.png)

## è®­ç»ƒæ–¹æ³•
![internvl-2.5-train](internvl-2.5-train.png)
æ¯ä¸ªæ¨¡å‹éƒ½åˆ†ä¸º3ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Œ3ä¸ªé˜¶æ®µéƒ½æ˜¯ä½¿ç”¨çš„**NTP Loss**
### stage-1: MLP WarmUp
åªæ›´æ–°MLPçš„å‚æ•°
### Stage-2: ViT Incremental Learning (Optional)
æ›´æ–°ViT + MLPçš„å‚æ•°
### Stage-3: Full Model Instruct Learning
æ›´æ–°æ•´ä¸ªæ¨¡å‹ï¼ˆViT + MLP + LLMï¼‰çš„å‚æ•°

### æ¸è¿‘ç¼©æ”¾ç­–ç•¥ï¼ˆProgressive Scaling Strategyï¼‰
åœ¨1.5å’Œ2.0éƒ½æåˆ°äº†ï¼Œè¯¥æ–¹æ³•æ¥è‡ªäº <u>â€œeven when the ViT and LLM are jointly trained using NTP loss, the resulting visual features are generalizable representations that can be easily understood by other LLMs.â€</u>çš„è§‚å¯Ÿ

ä½¿ç”¨Stage-1è®­ç»ƒå¥½1ä¸ªå‚æ•°é‡å°çš„MLLMçš„çš„InternViTï¼Œå¯ä»¥ç›´æ¥å°†è¯¥InternViTæ¥å…¥åˆ°1ä¸ªæ›´å¤§å‚æ•°é‡çš„MLLMçš„Stage-1.5é˜¶æ®µï¼Œä»è€Œä½¿å¾—æ›´å¤§å‚æ•°é‡çš„MLLMçœå»Stage-1çš„è®­ç»ƒï¼Œè¾¾åˆ°æå‡è®­ç»ƒçš„ç›®çš„ï¼›

ç„¶åè¿›è¡Œæ­£å¸¸çš„Stage-2çš„å…¨å‚å¾®è°ƒ

## è®­ç»ƒå¢å¼º
### éšæœºJPEGå‹ç¼©
å¯¹å›¾åƒè¿›è¡Œ75ï½100è´¨é‡çš„JPEGå‹ç¼©
### Loss Reweighting
å¸¸ç”¨çš„NTP LOSSé‡é‡‡æ ·æœ‰**Tokenå¹³å‡**å’Œ**æ ·æœ¬å¹³å‡**
![internvl-2.5-loss-reweighting](internvl-2.5-loss-reweighting.png)

tokenå¹³å‡ï¼š $\frac{1}{x^0}$æ’ç­‰äº1ï¼Œå³$w_i$æ’ç­‰äº1ï¼Œé‚£ä¹ˆæ¯ä¸ªtokenå¯¹NTP Lossçš„è´¡çŒ®éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¼šå¯¼è‡´æ¢¯åº¦åå·®åˆ°æ›´é•¿çš„tokenç”Ÿæˆä¸Š

æ ·æœ¬å¹³å‡ï¼š$\frac{1}{x^1}$ï¼Œä¼šç¡®ä¿æ¯ä¸ªæ ·æœ¬å¯¹ä¸NTP Lossçš„è´¡çŒ®æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯ä¼šå¯¼è‡´æ¨¡å‹æ›´åçˆ±äº§ç”Ÿæ›´çŸ­çš„response

ä¸ºäº†å‡è½»åœ¨è®­ç»ƒä¸­äº§ç”Ÿæ›´é•¿æˆ–è€…æ›´çŸ­çš„responseçš„åå·®ï¼Œæœ¬æ–‡é‡‡ç”¨square averagingçš„reweighting strategyï¼šå³ $w_i = \frac{1}{x^{0.5}}$

## å¤šæ¨¡æ€æ•°æ®PackingæŠ€æœ¯
åœ¨2.0å’Œ2.5çš„è®­ç»ƒä¸­ä½¿ç”¨ï¼Œç”¨äºå¢åŠ è®­ç»ƒæ•ˆç‡ï¼›

é€šè¿‡Selectã€Searchã€Packã€Maintainè¿™4ä¸ªé˜¶æ®µå®ç°å¤šæ¨¡æ€æ•°æ®çš„é«˜æ•ˆè®­ç»ƒï¼›

å¦‚æœ1ä¸ªæ ·æœ¬çš„tokenæ¯”è¾ƒçŸ­ï¼Œæ— æ³•å ç”¨LLMä¸€æ¬¡èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§tokenæ•°ç›®ï¼Œé‚£ä¹ˆå°±ä¼šåœ¨æ•°æ®é›†é‡Œé¢å†æœç´¢1ä¸ªtokenæ•°è¾ƒå°‘çš„æ ·æœ¬ï¼Œå°†å®ƒä»¬æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œåœ¨è®¡ç®—æ³¨æ„åŠ›çš„æ—¶å€™ï¼Œæ¯ä¸ªæ ·æœ¬åªä¼šå¯¹è‡ªå·±çš„tokenè®¡ç®—æ³¨æ„åŠ›

---
è®ºæ–‡è¿˜å‘ç°æ•°æ®è´¨é‡ç›¸æ¯”äºæ•°é‡æ›´é‡è¦ï¼›å¯¹äºæ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œèƒ½å¤Ÿå¸¦æ¥å¯è§‚çš„æ”¹å–„ï¼›

æœ¬æ–‡è§‚å¯Ÿåˆ°LLMæ¯”Vision Encoderå¯¹å™ªå£°æ›´æ•æ„Ÿï¼Œå³ä½¿ä¸€å°éƒ¨åˆ†çš„å¼‚å¸¸æ ·æœ¬å°±ä¼šåœ¨Stage-2çš„è®­ç»ƒä¸­å¯¼è‡´æ¨¡å‹å’Œç”¨æˆ·ä½“éªŒé€€åŒ–ã€‚åœ¨æ‰€æœ‰ç±»å‹çš„å¼‚å¸¸ä¸­ï¼Œé‡å¤ç”Ÿæˆæ˜¯æœ€ä¸¥é‡çš„é—®é¢˜ã€‚

åœ¨æ¯”è¾ƒéš¾çš„ä»»åŠ¡å¦‚MMMUä¸Šï¼Œåœ¨æ¨ç†çš„æ—¶å€™COTå¾ˆæœ‰ç”¨ï¼›è€Œä¸”è®ºæ–‡è¿˜éªŒè¯äº†COTå’Œå¤šæ•°æŠ•ç¥¨ä¸€èµ·ä½¿ç”¨æ•ˆæœæ›´å¥½ã€‚

# InterVL-2.5-MPO
å‘è¡¨äº2024.11.05

Paper: [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442)

Blog: [InternVL2.5-MPO: Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://internvl.github.io/blog/2024-12-20-InternVL-2.5-MPO/)

ä½¿ç”¨MPOä¹‹åæ¯”ä¸ä½¿ç”¨MPOçš„æ¨¡å‹åœ¨OpenCompass Leaderboardä¸Šå¹³å‡é«˜2ä¸ªç‚¹å·¦å³ã€‚

## MMPR
MMPRå³Multi-Modal Preference Datasetï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ä¾¿å¥½æ•°æ®æ„å»ºpipelineï¼ŒåŸºäºè¯¥pipelineæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡ã€å¤§è§„æ¨¡çš„ã€3Mçš„å¤šæ¨¡æ€æ¨ç†ä¾¿å¥½æ•°æ®é›†.

- å¯¹äºæœ‰æ˜ç¡®æ ‡å‡†ç­”æ¡ˆçš„æ ·æœ¬ï¼šæ¨¡å‹é¦–å…ˆè¢«promptè¾“å‡ºæ¨ç†è¿‡ç¨‹å’Œç”Ÿæˆæ ¼å¼ä¸º"Final Answer: xx"çš„æœ€ç»ˆç­”æ¡ˆï¼›å’Œgt matchçš„responseè¢«è®¤ä¸ºæ˜¯positive set $\mathcal{Y}_p$ , ä¸åŒ¹é…çš„è¢«è®¤ä¸ºæ˜¯negative set $\mathcal{Y}_n$ ,æ­¤å¤–é‚£äº›not clear responseä¹Ÿè¢«åˆå¹¶è¿› $\mathcal{Y}_n$ . å‡è®¾response labelè¢«è®¤ä¸ºæ˜¯positveå’Œnegativeï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡ä»$\mathcal{Y}_p$å’Œ$\mathcal{Y}_n$å„é€‰æ‹©1ä¸ªresponseä½œä¸ºpreference pair.
- å¯¹äºæ²¡æœ‰æ˜ç¡®æ ‡å‡†ç­”æ¡ˆçš„æ ·æœ¬ï¼šæå‡ºä¸€ä¸ªç®€å•é«˜æ•ˆçš„æ–¹æ³•ï¼šDropout NTP(Dropout Next Token Prediction), å…·ä½“çš„è®²ï¼šä½¿ç”¨InternVL2-8Bç”Ÿæˆçš„responseä½œä¸ºchosen answerã€‚å¯¹äºè¯¥chosen answer, åœ¨ä¸€åŠçš„åœ°æ–¹æˆªæ–­å®ƒï¼Œç„¶åpromptInternVL2-8Bå¯¹æˆªæ–­çš„ç­”æ¡ˆè¡¥å…¨ï¼ˆä¸ä½¿ç”¨å›¾åƒè¾“å…¥ï¼‰ï¼Œç”Ÿæˆçš„answerå°±ä½œä¸ºpaired sampleä¸­çš„reject answerã€‚å€¼å¾—æŒ‡å‡ºçš„æ˜¯ï¼šInternVL2-8Bç”Ÿæˆçš„ç­”æ¡ˆå¯èƒ½æ˜¯ä¸å®Œç¾çš„ï¼Œè€Œä¸”InternVL2-8Båœ¨ä¸ä½¿ç”¨å›¾åƒè¾“å…¥çš„æ—¶å€™è¡¥å…¨çš„ç­”æ¡ˆä¼šåŒ…å«æ›´å¤šçš„å¹»è§‰ï¼Œå› æ­¤æ‰èƒ½è´­åœ¨choosen answerå’Œreject answerä¹‹é—´çš„ååºå…³ç³»ä¿æŒä¸ºtrueã€‚

## MPO
MPO: Mixed Preference Optimization

MPOçš„å…³é”®åœ¨äºï¼šä¸€ä¸ªæœ‰æ•ˆçš„ PO æµç¨‹åº”å½“ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æˆå¯¹å“åº”ä¹‹é—´çš„ç›¸å¯¹åå¥½ã€å•ä¸ªå“åº”çš„ç»å¯¹è´¨é‡ä»¥åŠç”Ÿæˆæ›´ä¼˜å“åº”çš„è¿‡ç¨‹ã€‚

[//]: todo,è¡¥å……æŸå¤±çš„å½¢å¼
